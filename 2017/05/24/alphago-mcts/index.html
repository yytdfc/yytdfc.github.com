<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> AlphaGo - Searching with Policy and Value Networks · Votec</title><meta name="description" content="AlphaGo - Searching with Policy and Value Networks - ChenFu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://votec.top/atom.xml" title="Votec"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/images/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/tags/" target="_self" class="nav-list-link">TAGS</a></li><li class="nav-list-item"><a href="https://github.com/yytdfc" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">AlphaGo - Searching with Policy and Value Networks</h1><div class="post-info">May 24, 2017<full_date item.date ll>&#8853 <span id="busuanzi_container_page_pv">Viewed<span id="busuanzi_value_page_pv"></span> times</span></full_date></div><div class="post-content"><blockquote>
<p>本文是根据<em>AlphaGo</em>论文:<br>D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, D. Hassabis. <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" target="_blank" rel="external"><em>Mastering the Game of Go with Deep Neural Networks and Tree Search.</em></a> Nature 2016.<br>制作的一次<em>AlphaGo</em>算法培训的第二部分, 关于<em>AlphaGo</em>如果使用policy network和value network进行异步MCTS搜索, 对应的论文中的<em>4. Searching with Policy and Value Networks</em>.<br>欢迎提出宝贵意见.</p>
</blockquote>
<a id="more"></a>
<!-- $theme: default -->
<!-- $size: 16:9 -->
<hr>
<!-- page_number: true -->
<!-- footer: AlphaGo - Searching with Policy and Value Networks - ChenFu-->
<h2 id="AlphaGo-Overview"><a href="#AlphaGo-Overview" class="headerlink" title="AlphaGo Overview"></a><em><strong>AlphaGo Overview</strong></em></h2><ol>
<li>Supervised Learning of Policy Networks</li>
<li>Reinforcement Learning of Policy Networks</li>
<li>Reinforcement Learning of Value Networks</li>
<li><em><strong>Searching with Policy and Value Networks</strong></em></li>
</ol>
<hr>
<h2 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a><em><strong>Contents</strong></em></h2><ul>
<li>什么是 MCTS</li>
<li><em>AlphaGo</em> 中使用的 MCTS</li>
<li>APV-MCTS 实现细节</li>
</ul>
<hr>
<h2 id="MCTS-Monte-Carlo-Tree-Search"><a href="#MCTS-Monte-Carlo-Tree-Search" class="headerlink" title="MCTS (Monte-Carlo Tree Search)"></a>MCTS (Monte-Carlo Tree Search)</h2><ul>
<li>MCTS (Monte-Carlo Tree Search) 是一种用于某些决策过程的启发式搜索算法。</li>
<li>Rémi Coulom (Crazy Stone作者) 首先描述了蒙特卡洛方法在游戏树搜索的应用并命名为蒙特卡洛树搜索。</li>
<li><em>AlphaGo</em>出现前所有的顶尖围棋AI均采用 MCTS 方法。</li>
</ul>
<hr>
<h2 id="一般的树搜索"><a href="#一般的树搜索" class="headerlink" title="一般的树搜索"></a>一般的树搜索</h2><p><img src="/images/alphago/tree.webp" alt="logo center 60%"></p>
<ul>
<li>需要搜索的状态多，耗费计算量大</li>
</ul>
<hr>
<h2 id="MCTS"><a href="#MCTS" class="headerlink" title="MCTS"></a>MCTS</h2><p><img src="/images/alphago/mcts.webp" alt="logo center 60%"></p>
<ul>
<li>MCTS通过大量的随机模拟未来可能的棋局，不断逼近真实结果</li>
</ul>
<hr>
<h2 id="MCTS的四个步骤："><a href="#MCTS的四个步骤：" class="headerlink" title="MCTS的四个步骤："></a>MCTS的四个步骤：</h2><p><img src="/images/alphago/pure_mcts.webp" alt="logo center 100%"></p>
<hr>
<h2 id="AlphaGo中的MCTS"><a href="#AlphaGo中的MCTS" class="headerlink" title="AlphaGo中的MCTS"></a><em>AlphaGo</em>中的MCTS</h2><p><em>AlphaGo</em> 采用了 APV-MCTS (Asynchronous Policy and Value MCTS) 的方法：</p>
<ul>
<li>Asynchronous 异步搜索: CPU进行搜索和 $p<em>\pi$ 计算，GPU进行 $p</em>\sigma$ 和 $p_\theta$ 计算</li>
<li>Policy network: 给出下一步棋的可能的概率分布，减少搜索宽度</li>
<li>Value network: 给出某个搜索局面的评分，减少搜索深度</li>
</ul>
<hr>
<h2 id="使用Policy-network减小搜索宽度"><a href="#使用Policy-network减小搜索宽度" class="headerlink" title="使用Policy network减小搜索宽度"></a>使用Policy network减小搜索宽度</h2><p><img src="/images/alphago/policy.webp" alt="logo center 65%"></p>
<ul>
<li>更加有效的扩展树的分支，减少树的宽度</li>
</ul>
<hr>
<h2 id="使用Value-network减小搜索深度"><a href="#使用Value-network减小搜索深度" class="headerlink" title="使用Value network减小搜索深度"></a>使用Value network减小搜索深度</h2><p><img src="/images/alphago/value.webp" alt="logo center 62%"></p>
<ul>
<li>给出某个搜索局面的评分，在较少搜索树深度时得到更准确的局面评估</li>
</ul>
<hr>
<h2 id="APV-MCTS-的具体实现"><a href="#APV-MCTS-的具体实现" class="headerlink" title="APV-MCTS 的具体实现"></a>APV-MCTS 的具体实现</h2><p>&#160;　　　　 选择　　　　　　 扩展　　　　　　　　 评估　　　　　　 　　　 　回溯<br><img src="/images/alphago/3.webp" alt="logo center"></p>
<hr>
<h2 id="Step-1-Selection-选择"><a href="#Step-1-Selection-选择" class="headerlink" title="Step 1. Selection 选择"></a>Step 1. Selection 选择</h2><p>从根节点开始，根据$a<em>t=\arg\max\limits</em>{a}(Q(s_t,a)+u(s_t,a))$选择下一步走法，直到到达叶节点</p>
<ul>
<li><p>$Q(s,a)=(1-\lambda)\frac{W_v(s,a)}{N_v(s,a)}+\lambda\frac{W_r(s,a)}{N_r(s,a)}$　　　　$(\lambda=0.5)$<br>&#160;　　　　　　　　value评分　rollout评分</p>
</li>
<li><p>$u(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN_r(s,b)}}{1+N<em>r(s,b)}$　　　　$(c</em>{puct}=5)$<br>&#160;　　　　　　　 policy　　UCT</p>
</li>
</ul>
<hr>
<h2 id="Step-2-Expansion-扩展"><a href="#Step-2-Expansion-扩展" class="headerlink" title="Step 2. Expansion 扩展"></a>Step 2. Expansion 扩展</h2><p>当一个叶节点的访问次数使用$N<em>r(s,a)\gt n</em>{thr}$时，扩展该叶节点。$(n_{thr}=40)$</p>
<ul>
<li>首先在CPU上用tree policy $p_\tau(a|s’)$临时扩展下一步走法的概率分布</li>
<li><p>同时在GPU上计算SL policy $p<em>\sigma(a|s’)$，计算完成时替换$p</em>\tau$的结果</p>
</li>
<li><p>$P(s’,a)\leftarrow p_\sigma^\beta(a|s’)$<br>$(\beta$为softmax temperature, 使概率分布更加光滑, $\beta=0.67)$</p>
</li>
</ul>
<hr>
<h2 id="Step-3-Evaluation-评估"><a href="#Step-3-Evaluation-评估" class="headerlink" title="Step 3. Evaluation 评估"></a>Step 3. Evaluation 评估</h2><p>当选择到一个叶节点时，对当前的局面进行评估</p>
<ul>
<li>Value network 打分得到$v_\theta\in(-1,1)$ <em>(on GPU)</em></li>
<li>Rollout policy 模拟走子$a<em>t\sim p</em>\pi$到局终，结果$z_t\in{-1,0,1}$ <em>(on CPU)</em></li>
</ul>
<hr>
<h2 id="Step-4-Backup-回溯"><a href="#Step-4-Backup-回溯" class="headerlink" title="Step 4. Backup 回溯"></a>Step 4. Backup 回溯</h2><p>从该叶节点开始，向根节点回溯，迭代更新父节点评分值：</p>
<ul>
<li>$N_v(s,a)\leftarrow N_v(s,a)+1$, $W_v(s,a)\leftarrow W<em>v(s,a)+v</em>\theta$</li>
<li>$N_r(s,a)\leftarrow N_r(s,a)+1$, $W_r(s,a)\leftarrow W_r(s,a)+z_t$</li>
</ul>
<ul>
<li>$Q(s,a)=(1-\lambda)\frac{W_v(s,a)}{N_v(s,a)}+\lambda\frac{W_r(s,a)}{N_r(s,a)}$　　　　$(\lambda=0.5)$<br>&#160;　　　　　　　　value评分　rollout评分</li>
</ul>
<hr>
<h2 id="Step-5-End-结束"><a href="#Step-5-End-结束" class="headerlink" title="Step 5. End 结束"></a>Step 5. End 结束</h2><p>循环以上过程直至达到最大搜索次数或者设定时间，选择根节点下访问次数最多的走法。</p>
<p>若 $\max\limits_aQ(s,a)&lt;-0.8$，<em>AlphaGo</em>认输。</p>
<hr>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><table>
<thead>
<tr>
<th>Short name</th>
<th style="text-align:left">Policy network</th>
<th>Value network</th>
<th>Rollouts</th>
<th>Mixing constant</th>
<th>Policy GPUs</th>
<th>Value GPUs</th>
<th>Elo rating</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\alpha_{rvp}$</td>
<td style="text-align:left">$P_\sigma$</td>
<td>$v_\theta$</td>
<td>$P_\pi$</td>
<td>$\lambda=0.5$</td>
<td>2</td>
<td>6</td>
<td>2890</td>
</tr>
<tr>
<td>$\alpha_{vp}$</td>
<td style="text-align:left">$P_\sigma$</td>
<td>$v_\theta$</td>
<td>-</td>
<td>$\lambda=0$</td>
<td>2</td>
<td>6</td>
<td>2177</td>
</tr>
<tr>
<td>$\alpha_{rp}$</td>
<td style="text-align:left">$P_\sigma$</td>
<td>-</td>
<td>$P_\pi$</td>
<td>$\lambda=1$</td>
<td>8</td>
<td>0</td>
<td>2416</td>
</tr>
<tr>
<td>$\alpha_{rv}$</td>
<td style="text-align:left">[$P_\tau$]</td>
<td>$v_\theta$</td>
<td>$P_\pi$</td>
<td>$\lambda=0.5$</td>
<td>0</td>
<td>8</td>
<td>2077</td>
</tr>
<tr>
<td>$\alpha_{v}$</td>
<td style="text-align:left">[$P_\tau$]</td>
<td>$v_\theta$</td>
<td>-</td>
<td>$\lambda=0$</td>
<td>0</td>
<td>8</td>
<td>1655</td>
</tr>
<tr>
<td>$\alpha_{r}$</td>
<td style="text-align:left">[$P_\tau$]</td>
<td>-</td>
<td>$P_\pi$</td>
<td>$\lambda=1$</td>
<td>0</td>
<td>0</td>
<td>1457</td>
</tr>
<tr>
<td>$\alpha_{p}$</td>
<td style="text-align:left">$P_\sigma$</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0</td>
<td>0</td>
<td>1517</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="Elo-ratings-Nature-AlphaGo"><a href="#Elo-ratings-Nature-AlphaGo" class="headerlink" title="Elo ratings (Nature AlphaGo)"></a>Elo ratings (<em>Nature AlphaGo</em>)</h2><p><img src="/images/alphago/elo_nature.webp" alt="logo center 100%"></p>
<hr>
<h2 id="Elo-ratings-Seoul-AlphaGo"><a href="#Elo-ratings-Seoul-AlphaGo" class="headerlink" title="Elo ratings (Seoul AlphaGo)"></a>Elo ratings (<em>Seoul AlphaGo</em>)</h2><p><img src="/images/alphago/elo.webp" alt="logo center 63%"></p>
<hr>
<h2 id="AlphaGo-vs-Lee-Sedol-Game-1"><a href="#AlphaGo-vs-Lee-Sedol-Game-1" class="headerlink" title="AlphaGo vs Lee Sedol: Game 1"></a><em>AlphaGo</em> vs <em>Lee Sedol</em>: Game 1</h2><p><img src="/images/alphago/game1.webp" alt="logo center 63%"></p>
<hr>
<h2 id="AlphaGo-vs-Lee-Sedol-Game-2"><a href="#AlphaGo-vs-Lee-Sedol-Game-2" class="headerlink" title="AlphaGo vs Lee Sedol: Game 2"></a><em>AlphaGo</em> vs <em>Lee Sedol</em>: Game 2</h2><p><img src="/images/alphago/game2.webp" alt="logo center 63%"></p>
<hr>
<h2 id="AlphaGo-vs-Lee-Sedol-Game-3"><a href="#AlphaGo-vs-Lee-Sedol-Game-3" class="headerlink" title="AlphaGo vs Lee Sedol: Game 3"></a><em>AlphaGo</em> vs <em>Lee Sedol</em>: Game 3</h2><p><img src="/images/alphago/game3.webp" alt="logo center 63%"></p>
<hr>
<h2 id="AlphaGo-vs-Lee-Sedol-Game-4"><a href="#AlphaGo-vs-Lee-Sedol-Game-4" class="headerlink" title="AlphaGo vs Lee Sedol: Game 4"></a><em>AlphaGo</em> vs <em>Lee Sedol</em>: Game 4</h2><p><img src="/images/alphago/game4.webp" alt="logo center 63%"></p>
<hr>
<h2 id="AlphaGo-vs-Lee-Sedol-Game-5"><a href="#AlphaGo-vs-Lee-Sedol-Game-5" class="headerlink" title="AlphaGo vs Lee Sedol: Game 5"></a><em>AlphaGo</em> vs <em>Lee Sedol</em>: Game 5</h2><p><img src="/images/alphago/game5.webp" alt="logo center 63%"></p>
<hr>
<!-- page_number: false -->
<!-- footer:   -->
<h2 id="Thanks"><a href="#Thanks" class="headerlink" title="Thanks"></a><center>Thanks</center></h2></div></article></div></main><footer><div class="paginator"><a href="/2017/01/31/macOS-reinstall/" class="next">Next</a></div><div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div><script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script><script>var cloudTieConfig = {url: document.location.href,sourceId: "2017/05/24/alphago-mcts/",productKey: "b710aea8146b4b82be4291d960552145",target: "cloud-tie-wrapper"};
var yunManualLoad = true;
Tie.loader("aHR0cHM6Ly9hcGkuZ2VudGllLjE2My5jb20vcGMvbGl2ZXNjcmlwdC5odG1s", true);</script><div class="copyright"><p>© 2016~2017 <a href="http://votec.top">ChenFu</a>  &#8225  Site viewed <span id="busuanzi_value_site_pv"></span> times.</span></p><p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> + <a href="https://github.com/yytdfc/hexo-theme-apollo-plus" target="_blank">apollo</a> <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv"></p></div></footer></div><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" type="text/javascript"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-100648993-1','auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];
(function() {
var hm = document.createElement("script");
hm.src = "https://hm.baidu.com/hm.js?3a1457089b82f026e9a39fbe773531ff";
var s = document.getElementsByTagName("script")[0];
s.parentNode.insertBefore(hm, s);
})();</script><script type="text/javascript" src="http://tajs.qq.com/stats?sId=59104549" charset="UTF-8"></script></body></html>